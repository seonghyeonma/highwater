{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solved-alliance",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 코드 구성\n",
    "- 설정 - 라이브러리 로드 - 사용할 함수 정의 - 데이터 전처리 및 분리 - 모델 구성 - 훈련 - 추론\n",
    "\n",
    "# 요약\n",
    "## 설정\n",
    "- \"./vocab\", \"./model\" 폴더가 코드 실행에 필요합니다.\n",
    "- \"./dataset/\" 폴더에 데이콘에서 제공한 데이터가 있어야합니다.\n",
    "\n",
    "## 필요한 함수 정의\n",
    "- seed_everything\n",
    "    - 재현성을 위한 함수\n",
    "- text_preprocessing \n",
    "    - ip 패턴(ex 127.0.0.1) 혹은 ip.port(ex 127.0.0.1.8888) 를 모두 \"IP\"로 치환.\n",
    "    - 영어와 한글만 남김\n",
    "- return_vocab\n",
    "    - vocabulary 생성 함수\n",
    "    - 문장을 space단위로 분리 후 vocab을 생성\n",
    "- word2int\n",
    "    - word단위 token을 정수로 치환하여 정수 시퀀스를 만듦\n",
    "- padding\n",
    "    - 같은 길이로 만들어 주기 위해 뒷부분을 padding(0)으로 채움\n",
    "    - max_length(최대 길이)를 해당 노트북에서는 200으로 함.\n",
    "- return_similarity\n",
    "    - 텍스트 유사도 계산\n",
    "- level\n",
    "    - threshold 값 이상인 값이 없으면 level 7 반환\n",
    "    \n",
    "## 전처리\n",
    "- 처음 데이터를 볼 때 비정상적으로 긴 길이의 log인 452419를 제외 하였음. 하지만 영어와 한글만 남기는 전처리를 거치면 사용해도 상관 없을것으로 보임\n",
    "- 치환, 영어와 한글만 남기는 전처리 후 중복되는 데이터는 하나만 남김.\n",
    "- level 2,4,6은 샘플 수가 저기 떄문에 중복되는 텍스트라도 제거하지 않고 validation성능 측정 때 사용되도록 함.\n",
    "\n",
    "## 텍스트 유사도\n",
    "- 숫자를 제외한 영어와 한글만 남겨 전처리 했을 때 한 단어 차이로 level이 다른 샘플이 존재함.\n",
    "- 제공된 데이터 전처리한 결과들끼리 자카드 유사도를 구하여 0.5이상인데 level이 다르면 무조건 훈련 샘플로 사용할 수 있도록 해당 index를 저장해둠\n",
    "- 60818 * 60181 번 loop가 실행되기 떄문에 시간이 꽤 오래 걸림.\n",
    "\n",
    "## 훈련/검증 데이터 분리\n",
    "- 5-fold로 분리\n",
    "- 각 폴드에서 텍스트 유사도가 높았던 index는 훈련샘플에는 넣고 검증 샘플에서는 제거.\n",
    "\n",
    "## Multiple-Kernel Size CNNs-BiLSTM\n",
    "- 각각 다른 kernel size를 가진 Conv를 통해 시퀀스에서 패턴을 추출하고 LSTM을 통해 순서정보를 학습하는 것을 기대하고 모델을 구성하였습니다.\n",
    "- 전처리된 정수 시퀀스가 모델에 입력.(x)\n",
    "- 입력된 정수 시퀀스는 Embedding Layer를 통해 각 정수들은 1024 차원으로 인코딩되어 벡터 시퀀스로 변환.(nn.Embedding)\n",
    "- 벡터 시퀀스는 kernel size가 다른 6개의 Conv로 입력(nn.Conv1d)되고 출력된 feature map은 Maxpooling.(nn.MaxPool1d)\n",
    "- 6개의 CNN에서 출력된 값들을 이어준 후 (torch.cat) BatchNorm을 적용.\n",
    "- BiLSTM에 입력(nn.LSTM)되고 출력된 값들을 average pooling 한후 squeeze 해줌.\n",
    "- 마지막 출력은 class 개수(7)로 출력.\n",
    "\n",
    "## Training \n",
    "- 5개 폴드\n",
    "- Epoch : 30\n",
    "- batchSize : 400\n",
    "- optimizer : adam\n",
    "- scheduler : CosineAnnealingLR\n",
    "- loss : CrossEntropy\n",
    "- save : validation f1-macro\n",
    "- 훈련시 confidence가 0.85 이상인 것이 없으면 7로 분류하여 성능평가하였음.\n",
    "\n",
    "## Test inference\n",
    "- 훈련 데이터와 같은 방법으로 전처리. vocab은 훈련데이터에서 만든 vocab을 사용.\n",
    "- 5-fold 훈련을 했지만 validation 성능이 좋지 않았던 4번 째 fold로 만든 모델은 test inference로 사용하지 않음.\n",
    "- 4개 모델로 예측한 값들을 평균내어 confidence가 0.7 이상인 것이 없으면 level 7로 반환 하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-shoot",
   "metadata": {},
   "source": [
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-thesaurus",
   "metadata": {},
   "source": [
    "# 설정\n",
    "- \"./vocab\", \"./model\" 폴더가 필요\n",
    "- 데이콘에서 제공된 데이터셋은 \"./dataset\" 에 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-wells",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T18:50:34.487222Z",
     "start_time": "2021-05-14T18:50:34.482624Z"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir vocab\n",
    "# !mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498361be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch_poly_lr_decay (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch_poly_lr_decay\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_poly_lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-enterprise",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "measured-exclusive",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:38:21.692456Z",
     "start_time": "2021-05-14T19:38:20.473887Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-fellow",
   "metadata": {},
   "source": [
    "# 필요한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "married-auction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:12:13.340404Z",
     "start_time": "2021-05-14T20:12:13.316560Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "# 텍스트 전처리. \n",
    "# ip 패턴(ex 127.0.0.1) 혹은 ip.port(ex 127.0.0.1.8888) 를 모두 \"IP\"로 치환.\n",
    "# 영어와 한글만 남김ㅅ\n",
    "def text_preprocessing(texts):\n",
    "    result=[]\n",
    "    for log in tqdm(texts):\n",
    "        log = log.replace('\\\\', '')\n",
    "        log = re.sub(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d{1,5}\\b\", 'IP', log)\n",
    "        log = re.sub(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", 'IP', log)\n",
    "        eng_kor = re.compile('[^a-zA-Zㄱ-ㅣ가-힣]+') \n",
    "        log = eng_kor.sub(' ', log)\n",
    "        result.append(log)\n",
    "    return result\n",
    "\n",
    "# vocabulary 생성 함수\n",
    "# 문장을 space단위로 분리 후 vocab을 생성\n",
    "def return_vocab(texts):\n",
    "    vocab = set()\n",
    "    for text in tqdm(texts):\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            vocab.add(word)\n",
    "    vocab = list(vocab)\n",
    "    vocab = {word:i for word,i in zip(vocab, range(len(vocab)))}\n",
    "\n",
    "    vocab_count = {w:0 for w in vocab}\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            vocab_count[word]+=1\n",
    "\n",
    "    vocab = {}\n",
    "    idx=0\n",
    "    for word, cnt in tqdm(zip(vocab_count.keys(), vocab_count.values())):\n",
    "        if cnt>=1:\n",
    "            vocab[word] = idx+1\n",
    "            idx+=1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# word 단위 token을 정수로 치환하여 정수 시퀀스를 만듦\n",
    "def word2int(li, vocab, train=False):\n",
    "    result=[]\n",
    "    for text in tqdm(li):\n",
    "        tmp = []\n",
    "        text = text.split()\n",
    "        for word in text:\n",
    "            if word in vocab.keys():\n",
    "                tmp.append(vocab[word]+1)\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 같은 길이로 만들어 주기 위해 뒷부분을 padding(0)으로 채움\n",
    "# max_length(최대 길이)는 해당 노트북에서는 200으로 함.\n",
    "def padding(sequences, max_length):\n",
    "    result=[]\n",
    "    for sequence in tqdm(sequences):\n",
    "        if len(sequence) < max_length:\n",
    "            sequence +=[0]*(max_length-len(sequence))\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "        result.append(sequence)\n",
    "    return np.array(result)\n",
    "\n",
    "# 텍스트 유사도 계산\n",
    "def return_similarity(a, b): \n",
    "    c = a.intersection(b)\n",
    "    return float(len(c))/(len(a)+len(b)-len(c))\n",
    "\n",
    "\n",
    "\n",
    "def softmax(k):\n",
    "    exp_k = np.exp(k)\n",
    "    sum_exp_k = np.sum(exp_k)\n",
    "    y = exp_k / sum_exp_k\n",
    "    return y\n",
    "\n",
    "\n",
    "# threshold 값 이상인 값이 없으면 7로 반환\n",
    "def level(array, threshold):\n",
    "    if (array>=threshold).any() == True:\n",
    "        return array.argmax()\n",
    "    else:\n",
    "        return 7\n",
    "    \n",
    "\n",
    "def return_level_array(array, threshold, count_print=False):\n",
    "    result = np.array([level(k,threshold) for k in array])\n",
    "    if count_print==True:\n",
    "        print(f'threshold : {threshold}              ',np.unique(result, return_counts=True)[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "def model_save(model, path):\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-bankruptcy",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "- 처음 데이터를 볼 당시 비 정상적으로 긴 길이의 log가 나와서 452419를 제외 --> 영어와 한글만 남기는 전처리를 거치면 사용해도 상관없을것으로 보임\n",
    "- 치환, 영어와 한글만 남기는 전처리 후 중복되는 텍스트는 하나만 남김. \n",
    "- level 2,4,6은 샘플 수가 적기 때문에 중복되는 텍스트라도 제거하지 않고 validation 성능 측정 때 사용되도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd9b48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576409</td>\n",
       "      <td>StartLoanApply</td>\n",
       "      <td>2022-03-25 11:12:09</td>\n",
       "      <td>2022-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576409</td>\n",
       "      <td>ViewLoanApplyIntro</td>\n",
       "      <td>2022-03-25 11:12:09</td>\n",
       "      <td>2022-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72878</td>\n",
       "      <td>EndLoanApply</td>\n",
       "      <td>2022-03-25 11:14:44</td>\n",
       "      <td>2022-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>645317</td>\n",
       "      <td>OpenApp</td>\n",
       "      <td>2022-03-25 11:15:09</td>\n",
       "      <td>2022-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>645317</td>\n",
       "      <td>UseLoanManage</td>\n",
       "      <td>2022-03-25 11:15:11</td>\n",
       "      <td>2022-03-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id               event            timestamp     date_cd\n",
       "0   576409      StartLoanApply  2022-03-25 11:12:09  2022-03-25\n",
       "1   576409  ViewLoanApplyIntro  2022-03-25 11:12:09  2022-03-25\n",
       "2    72878        EndLoanApply  2022-03-25 11:14:44  2022-03-25\n",
       "3   645317             OpenApp  2022-03-25 11:15:09  2022-03-25\n",
       "4   645317       UseLoanManage  2022-03-25 11:15:11  2022-03-25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('log_data_clean.csv')\n",
    "dataset = dataset.drop('Unnamed: 0', axis = 1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0e59aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('log_data_clean.csv')\n",
    "dataset = dataset.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[['user_id','timestamp',\n",
    "       'date_cd']], dataset['event'])\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-coaching",
   "metadata": {},
   "source": [
    "## 텍스트 유사도\n",
    "- 숫자를 제외한 영어와 한글만 남겨 전처리 했을 때 한 단어 차이로 level이 다른 샘플이 존재함.\n",
    "- 제공된 데이터 전처리한 결과들 끼리 자카드 유사도를 구하여 유사도가 0.5 이상인데 level이 다르면 무조건 훈련샘플로 사용하기 위해 해당 index를 저장해둠.\n",
    "- 60818 * 60818 번 loop 가 실행되기 때문에 시간이 오래걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-natural",
   "metadata": {},
   "source": [
    "## 훈련/검증  데이터 분리\n",
    "- 5-fold로 분리\n",
    "- 각 폴드에서 텍스트 유사도가 높던것은 훈련 샘플로 넣고 검증 샘플로 들어간것은 제거.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-madness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:38:46.147420Z",
     "start_time": "2021-05-14T19:38:46.096088Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(TRAIN, Y_train):\n",
    "    train_idx = np.array(list(set(list(train_idx)+list(sim_idx))))\n",
    "    valid_idx = np.array(list(set(set(valid_idx)-set(sim_idx))))\n",
    "    folds.append((train_idx, valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-actress",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "- 로더에서는 다른 처리는 없고 배치단위로 불러오기만 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mechanical-spank",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:38:46.151998Z",
     "start_time": "2021-05-14T19:38:46.148550Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, X, y=None, train=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.train=train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        if self.train==True:\n",
    "            y = self.y[idx]\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-surrey",
   "metadata": {},
   "source": [
    "# Multiple-Kernel Size CNNs-BiLSTM\n",
    "#### 각각 다른 kernel size를 가진 Conv를 통해 시퀀스에서 패턴을 추출하고 LSTM을 통해 순서정보를 학습하는 것을 기대하고 모델을 구성하였습니다.\n",
    "\n",
    "- 전처리된 정수 시퀀스가 모델에 입력.(x)\n",
    "- 입력된 정수 시퀀스는 Embedding Layer를 통해 각 정수들은 1024 차원으로 인코딩되어 벡터 시퀀스로 변환.(nn.Embedding)\n",
    "- 벡터 시퀀스는 kernel size가 다른 6개의 Conv로 입력(nn.Conv1d)되고 출력된 feature map은 Maxpooling.(nn.MaxPool1d)\n",
    "- 6개의 CNN에서 출력된 값들을 이어준 후 (torch.cat) BatchNorm을 적용.\n",
    "- BiLSTM에 입력(nn.LSTM)되고 출력된 값들을 average pooling 한후 squeeze 해줌.\n",
    "- 마지막 출력은 class 개수(7)로 출력.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alike-distribution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:38:46.252685Z",
     "start_time": "2021-05-14T19:38:46.153052Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Log_classification(nn.Module):\n",
    "    def __init__(self, b=None):\n",
    "        super(Log_classification, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(len(vocab)+3, embedding_dim=1024, padding_idx=0)\n",
    "        \n",
    "        # Conv\n",
    "        self.conv1 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=6, padding=3)\n",
    "        self.conv4 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=8, padding=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=10, padding=5)\n",
    "        self.conv6 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=12, padding=6)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=100, hidden_size=32, bidirectional=True, num_layers=1, dropout=0.3)\n",
    "        \n",
    "        # Pooling\n",
    "        self.maxpooling = nn.MaxPool1d(2)\n",
    "        self.average_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # BatchNormalization\n",
    "        self.BN1 = nn.BatchNorm1d(512*6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Linear(512*6, 7)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, return_mid=False):\n",
    "        x_ = self.embedding(x)\n",
    "        x_ = torch.movedim(x_, 1, 2)\n",
    "        \n",
    "        \n",
    "        conv_k1_x = F.elu(self.conv1(x_))\n",
    "        conv_k1_x = self.maxpooling(conv_k1_x)\n",
    "        \n",
    "        conv_k2_x = F.elu(self.conv2(x_))\n",
    "        conv_k2_x = self.maxpooling(conv_k2_x)\n",
    "        \n",
    "        conv_k3_x = F.elu(self.conv3(x_))\n",
    "        conv_k3_x = self.maxpooling(conv_k3_x)\n",
    "        \n",
    "        conv_k4_x = F.elu(self.conv4(x_))\n",
    "        conv_k4_x = self.maxpooling(conv_k4_x)\n",
    "        \n",
    "        conv_k5_x = F.elu(self.conv5(x_))\n",
    "        conv_k5_x = self.maxpooling(conv_k5_x)\n",
    "        \n",
    "        conv_k6_x = F.elu(self.conv6(x_))\n",
    "        conv_k6_x = self.maxpooling(conv_k6_x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        conv_x = torch.cat([conv_k1_x, conv_k2_x, conv_k3_x, conv_k4_x, conv_k5_x, conv_k6_x], 1)\n",
    "        conv_x_ = self.BN1(conv_x)\n",
    "        conv_x = self.dropout(conv_x_)\n",
    "        \n",
    "        lstm, _ = self.lstm1(conv_x)\n",
    "        lstm = self.average_pooling(lstm)\n",
    "        lstm = torch.squeeze(lstm)\n",
    "        return_value = lstm\n",
    "\n",
    "        output = (self.output(lstm))\n",
    "        if return_mid == True:\n",
    "            return output, return_value\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-library",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 5개 폴드\n",
    "- Epoch : 30\n",
    "- batchSize : 400\n",
    "- optimizer : adam\n",
    "- scheduler : CosineAnnealingLR\n",
    "- loss : CrossEntropy\n",
    "- save : validation f1-macro\n",
    "- 훈련시 confidence가 0.85 이상인 것이 없으면 7로 분류하여 성능평가하였음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "systematic-elite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:10:19.274084Z",
     "start_time": "2021-05-14T18:51:00.085006Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/30    TRAIN -> loss : 0.0892  f1-macro : 0.37727993427702966    //    VALID -> loss : 0.0133  f1-macro : 0.5286556202378457  time : 45/1336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.74      0.85        70\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       1.00      0.86      0.92       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.98      0.99     11888\n",
      "   macro avg       0.57      0.49      0.53     11888\n",
      "weighted avg       1.00      0.98      0.99     11888\n",
      "\n",
      "[[  664     0     0     0     0     0     0   106]\n",
      " [    1 10888     0     0     0     0     0    39]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0    52     0     0     0    18]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0    98     0    16]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 2/30    TRAIN -> loss : 0.0219  f1-macro : 0.5313435984675694    //    VALID -> loss : 0.0097  f1-macro : 0.5502496911259439  time : 37/1087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.93      0.96        70\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       1.00      0.92      0.96       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      0.99     11888\n",
      "   macro avg       0.57      0.53      0.55     11888\n",
      "weighted avg       1.00      0.99      0.99     11888\n",
      "\n",
      "[[  672     0     0     0     0     0     0    98]\n",
      " [    1 10895     0     0     0     0     0    32]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0    65     0     0     0     5]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0   105     0     9]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 3/30    TRAIN -> loss : 0.0142  f1-macro : 0.5725712046032955    //    VALID -> loss : 0.0089  f1-macro : 0.7002027245268014  time : 38/1055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.94      0.97        70\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.71      0.69      0.70     11888\n",
      "weighted avg       1.00      0.99      0.99     11888\n",
      "\n",
      "[[  692     0     0     0     0     0     0    78]\n",
      " [    2 10902     0     0     0     0     0    24]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    0     0     0    66     0     0     0     4]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0   111     0     3]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 4/30    TRAIN -> loss : 0.0097  f1-macro : 0.6896332018406738    //    VALID -> loss : 0.0090  f1-macro : 0.7012974330312881  time : 38/1022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.96      0.98        70\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.71      0.69      0.70     11888\n",
      "weighted avg       1.00      0.99      0.99     11888\n",
      "\n",
      "[[  694     1     0     0     0     0     0    75]\n",
      " [    4 10903     0     0     0     0     0    21]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    0     0     0    67     0     0     0     3]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0   111     0     3]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 5/30    TRAIN -> loss : 0.0083  f1-macro : 0.6912322048242757    //    VALID -> loss : 0.0089  f1-macro : 0.7970763734699297  time : 38/984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.96      0.98        70\n",
      "           4       1.00      0.50      0.67         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.86      0.76      0.80     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  700     0     0     0     0     0     0    70]\n",
      " [    5 10905     0     0     0     0     0    18]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    0     0     0    67     0     0     0     3]\n",
      " [    0     0     0     0     1     0     0     1]\n",
      " [    0     0     0     0     0   111     0     3]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 6/30    TRAIN -> loss : 0.0085  f1-macro : 0.7873695074105502    //    VALID -> loss : 0.0097  f1-macro : 0.8461751174030985  time : 38/949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.97      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.86      0.84      0.85     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  705     1     0     0     0     0     0    64]\n",
      " [    6 10907     0     0     0     0     0    15]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    0     0     0    68     0     0     0     2]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    0     0     0     0     0   111     0     3]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 7/30    TRAIN -> loss : 0.0094  f1-macro : 0.8254108131685033    //    VALID -> loss : 0.0097  f1-macro : 0.8469748751195343  time : 38/910\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.96      0.98        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.85      0.84      0.85     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  733     1     0     0     0     0     0    36]\n",
      " [   12 10906     0     0     0     0     0    10]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    67     0     0     0     2]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     0     2]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 8/30    TRAIN -> loss : 0.0112  f1-macro : 0.8776727196865026    //    VALID -> loss : 0.0100  f1-macro : 0.9897219943447152  time : 38/875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.97      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  715     7     0     0     0     0     0    48]\n",
      " [    7 10907     0     0     0     0     0    14]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    68     0     0     0     1]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    1     0     0     0     0   111     0     2]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9/30    TRAIN -> loss : 0.0073  f1-macro : 0.9348055807043478    //    VALID -> loss : 0.0111  f1-macro : 0.9882816635739403  time : 38/834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.96      0.98        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  713     8     0     0     0     0     0    49]\n",
      " [    8 10908     0     0     0     0     0    12]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    67     0     0     0     2]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    2     0     0     0     0   111     0     1]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 10/30    TRAIN -> loss : 0.0060  f1-macro : 0.9582526501298249    //    VALID -> loss : 0.0112  f1-macro : 0.9903645443500154  time : 38/798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.98      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.99      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  728     5     0     0     0     1     0    36]\n",
      " [   10 10907     0     0     0     1     0    10]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 11/30    TRAIN -> loss : 0.0030  f1-macro : 0.9893674163179614    //    VALID -> loss : 0.0126  f1-macro : 0.9910293165120881  time : 38/759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  717    13     0     0     0     0     0    40]\n",
      " [    6 10909     0     0     0     0     0    13]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    1     0     0     0     0   111     0     2]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 12/30    TRAIN -> loss : 0.0013  f1-macro : 0.9912837141495084    //    VALID -> loss : 0.0125  f1-macro : 0.9914894003284983  time : 38/723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  728    11     0     0     0     0     0    31]\n",
      " [   11 10909     0     0     0     0     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 13/30    TRAIN -> loss : 0.0008  f1-macro : 0.9917053074642659    //    VALID -> loss : 0.0126  f1-macro : 0.9914894003284983  time : 38/683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  728    11     0     0     0     0     0    31]\n",
      " [   11 10909     0     0     0     0     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 14/30    TRAIN -> loss : 0.0008  f1-macro : 0.9915236017969622    //    VALID -> loss : 0.0129  f1-macro : 0.9913848292093608  time : 38/645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  727    12     0     0     0     0     0    31]\n",
      " [   11 10909     0     0     0     0     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 15/30    TRAIN -> loss : 0.0012  f1-macro : 0.9916507445153636    //    VALID -> loss : 0.0135  f1-macro : 0.9924815786806153  time : 38/609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      1.00      1.00     11888\n",
      "\n",
      "[[  739     9     0     0     0     0     0    22]\n",
      " [   12 10909     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 16/30    TRAIN -> loss : 0.0016  f1-macro : 0.9845374396295747    //    VALID -> loss : 0.0136  f1-macro : 0.9915877541201635  time : 38/569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  730    12     0     0     0     0     0    28]\n",
      " [   12 10909     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17/30    TRAIN -> loss : 0.0031  f1-macro : 0.9905829326193193    //    VALID -> loss : 0.0141  f1-macro : 0.9904645686003507  time : 38/531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  715    15     0     0     0     0     0    40]\n",
      " [    8 10911     0     0     0     0     0     9]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 18/30    TRAIN -> loss : 0.0033  f1-macro : 0.9900099198999736    //    VALID -> loss : 0.0131  f1-macro : 0.9902540417725387  time : 38/493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.98      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.99      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  728     9     0     0     0     1     0    32]\n",
      " [   11 10908     0     0     0     1     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 19/30    TRAIN -> loss : 0.0019  f1-macro : 0.9908261950881425    //    VALID -> loss : 0.0136  f1-macro : 0.990826520620305  time : 38/455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.98      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       0.99      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  734    11     0     0     0     1     0    24]\n",
      " [   11 10908     0     0     0     1     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 20/30    TRAIN -> loss : 0.0010  f1-macro : 0.991673309845365    //    VALID -> loss : 0.0148  f1-macro : 0.9912644749341041  time : 38/418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  723    15     0     0     0     0     0    32]\n",
      " [    8 10912     0     0     0     0     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 21/30    TRAIN -> loss : 0.0004  f1-macro : 0.9923031754182209    //    VALID -> loss : 0.0148  f1-macro : 0.9914828894749265  time : 38/379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  728    13     0     0     0     0     0    29]\n",
      " [   11 10910     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 22/30    TRAIN -> loss : 0.0003  f1-macro : 0.992370552967311    //    VALID -> loss : 0.0148  f1-macro : 0.9915808016749986  time : 38/343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  729    13     0     0     0     0     0    28]\n",
      " [   11 10910     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 23/30    TRAIN -> loss : 0.0003  f1-macro : 0.9923432680956606    //    VALID -> loss : 0.0150  f1-macro : 0.9919710521220159  time : 38/304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      1.00      1.00     11888\n",
      "\n",
      "[[  734    13     0     0     0     0     0    23]\n",
      " [   12 10909     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 24/30    TRAIN -> loss : 0.0004  f1-macro : 0.9923191875503197    //    VALID -> loss : 0.0157  f1-macro : 0.9927641710241701  time : 38/266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      1.00      1.00     11888\n",
      "\n",
      "[[  742    10     0     0     0     0     0    18]\n",
      " [   12 10909     0     0     0     0     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25/30    TRAIN -> loss : 0.0007  f1-macro : 0.9854641130319836    //    VALID -> loss : 0.0183  f1-macro : 0.9905343578973852  time : 38/227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  714    20     0     0     0     0     0    36]\n",
      " [    6 10914     0     0     0     0     0     8]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 26/30    TRAIN -> loss : 0.0020  f1-macro : 0.9915022388498935    //    VALID -> loss : 0.0190  f1-macro : 0.9879040253206651  time : 38/190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.97      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.98      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.98      0.99     11888\n",
      "weighted avg       1.00      0.99      0.99     11888\n",
      "\n",
      "[[  711    26     0     0     0     1     0    32]\n",
      " [    7 10913     0     0     0     1     0     7]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    68     0     0     0     1]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    2     0     0     0     0   111     0     1]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 27/30    TRAIN -> loss : 0.0024  f1-macro : 0.9842184853648828    //    VALID -> loss : 0.0159  f1-macro : 0.9912662865036957  time : 38/152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.97      0.99       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  725    18     0     0     0     0     0    27]\n",
      " [   10 10913     0     0     0     0     0     5]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 28/30    TRAIN -> loss : 0.0014  f1-macro : 0.9847896331660307    //    VALID -> loss : 0.0161  f1-macro : 0.9914774114089263  time : 38/114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.99      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      1.00      1.00     11888\n",
      "\n",
      "[[  734     9     0     0     0     0     0    27]\n",
      " [   11 10911     0     0     0     1     0     5]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 29/30    TRAIN -> loss : 0.0004  f1-macro : 0.9922308293859395    //    VALID -> loss : 0.0170  f1-macro : 0.9909832858691933  time : 38/76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.99      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  729    12     0     0     0     0     0    29]\n",
      " [   11 10913     0     0     0     1     0     3]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n",
      "Epoch : 30/30    TRAIN -> loss : 0.0002  f1-macro : 0.992434714148252    //    VALID -> loss : 0.0172  f1-macro : 0.9909702288443648  time : 38/38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       770\n",
      "           1       1.00      1.00      1.00     10928\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.99      0.99        70\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.99      0.97      0.98       114\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      0.99      1.00     11888\n",
      "   macro avg       1.00      0.99      0.99     11888\n",
      "weighted avg       1.00      0.99      1.00     11888\n",
      "\n",
      "[[  729    14     0     0     0     0     0    27]\n",
      " [   11 10913     0     0     0     1     0     3]\n",
      " [    0     0     2     0     0     0     0     0]\n",
      " [    1     0     0    69     0     0     0     0]\n",
      " [    0     0     0     0     2     0     0     0]\n",
      " [    3     0     0     0     0   111     0     0]\n",
      " [    0     0     0     0     0     0     2     0]\n",
      " [    0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "seed_everything(random_state)\n",
    "\n",
    "for fold in range(5):\n",
    "    save_dir = 'model'\n",
    "    model_name = f'max_length({max_length})-random_state({random_state})-fold({fold+1})'\n",
    "    if fold==0:\n",
    "        with open(f'vocab/{model_name}.bin', 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "\n",
    "    epochs=30\n",
    "    batch_size=400\n",
    "    best = 0\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "\n",
    "    train_dataset = LogDataset(TRAIN[train_idx], Y_train[train_idx])\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    valid_dataset = LogDataset(TRAIN[valid_idx], Y_train[valid_idx])\n",
    "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # Build model\n",
    "    model = Log_classification().to(device)\n",
    "    model = nn.DataParallel(model, device_ids=[0,1,2])\n",
    "\n",
    "    \n",
    "    # Optimizer\n",
    "    Q = math.floor(len(train_dataset)/batch_size+1)*epochs/7\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    lrs = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = Q)\n",
    "    \n",
    "    \n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start=time.time()\n",
    "        \n",
    "        ####################################################### TRAIN\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        train_pred=[]\n",
    "        train_true=[]\n",
    "\n",
    "        for X, y in (train_loader):\n",
    "            X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "            y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lrs.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_pred += list(pred.detach().cpu().numpy())\n",
    "            train_true += list(y.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        train_pred = np.array([level(softmax(k), 0.85) for k in train_pred])\n",
    "        train_f1_macro = f1_score(train_true, train_pred, average='macro', labels=[0,1,2,3,4,5,6])\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "\n",
    "        ####################################################### VALIDATION\n",
    "        model.eval()\n",
    "        valid_loss=0\n",
    "        valid_pred=[]\n",
    "        valid_true=[]\n",
    "\n",
    "        for X, y in (valid_loader):\n",
    "            X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "            y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            valid_pred += list(pred.detach().cpu().numpy())\n",
    "            valid_true += list(y.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        valid_pred = np.array([level(softmax(k), 0.85) for k in valid_pred])        \n",
    "        valid_f1_macro = f1_score(valid_true, valid_pred, average='macro', labels=[0,1,2,3,4,5,6])\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "        \n",
    "        \n",
    "        running_time=time.time()-start\n",
    "        print(f'Epoch : {epoch+1}/{epochs}    TRAIN -> loss : {train_loss:.4f}  f1-macro : {train_f1_macro}    //    VALID -> loss : {valid_loss:.4f}  f1-macro : {valid_f1_macro}  time : {running_time:.0f}/{(epochs-epoch)*running_time:.0f}')\n",
    "        print(classification_report(valid_true, valid_pred, labels=[0,1,2,3,4,5,6]))\n",
    "        print(confusion_matrix(valid_true, valid_pred, labels=[0,1,2,3,4,5,6,7]))\n",
    "        if valid_f1_macro>=best:\n",
    "            best = valid_f1_macro\n",
    "            model_save(model=model, path=f'{save_dir}/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-ready",
   "metadata": {},
   "source": [
    "# level7-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "another-friend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:39:45.875271Z",
     "start_time": "2021-05-14T19:39:45.870578Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = sorted(glob('model/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "smooth-class",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:41:34.593887Z",
     "start_time": "2021-05-15T06:41:33.268384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7092.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14429.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 36366.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2597.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5499.52it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 25890.77it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8496.23it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14751.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 35345.26it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2734.23it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5354.43it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15215.13it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7543.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14217.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 34473.73it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f'vocab/max_length({max_length})-random_state({random_state})-fold(1).bin', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "weights = sorted(glob('model/*'))\n",
    "level7_df = pd.read_csv('dataset/validation_sample.csv')\n",
    "\n",
    "\n",
    "level7_preds = 0\n",
    "for w in weights:\n",
    "    \n",
    "    model=Log_classification().to(device)\n",
    "    model = nn.DataParallel(model, device_ids=[0])\n",
    "    \n",
    "    \n",
    "    weight = torch.load(w)\n",
    "    model.load_state_dict(weight['model'])\n",
    "    model.eval()\n",
    "\n",
    "    level7=pd.concat([level7_df])\n",
    "    level7_log = text_preprocessing(level7['full_log'].values)\n",
    "    level7_log = word2int(level7_log, vocab)\n",
    "    level7_log = padding(level7_log, max_length=max_length)\n",
    "    level7_log = torch.tensor(level7_log, dtype=torch.long, device=device)\n",
    "    level7_log = model(level7_log).detach().cpu().numpy()\n",
    "    level7_log = level7_log[:3]\n",
    "    level7_log = (np.array([softmax(k) for k in level7_log]).round(3))\n",
    "    level7_preds += level7_log/5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-pound",
   "metadata": {},
   "source": [
    "# Test inference\n",
    "\n",
    "- 훈련 데이터와 같은 방법으로 전처리. vocab은 훈련데이터에서 만든 vocab을 사용\n",
    "- 5-fold 훈련을 했지만 validation 성능이 좋지 않았던 4번 째 fold로 만든 모델은 test inference로 사용하지 않음.\n",
    "- 4개 모델로 예측한 값들을 평균내어 confidence가 0.7 이상인 것이 없으면 level 7로 반환 하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "realistic-jonathan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:41:29.635046Z",
     "start_time": "2021-05-14T19:39:49.804742Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418916/1418916 [00:55<00:00, 25337.98it/s]\n",
      "100%|██████████| 1418916/1418916 [00:24<00:00, 58383.73it/s]\n",
      "100%|██████████| 1418916/1418916 [00:02<00:00, 560883.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f'vocab/max_length({max_length})-random_state({random_state})-fold(1).bin', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "test_log = text_preprocessing(test['full_log'].values)\n",
    "TEST = word2int(test_log, vocab, train=True)\n",
    "TEST = padding(TEST, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "armed-tribe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:41:29.641358Z",
     "start_time": "2021-05-14T19:41:29.637213Z"
    }
   },
   "outputs": [],
   "source": [
    "def output(model_name):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    save_dir = 'model'\n",
    "    model = Log_classification().to(device)\n",
    "    model = nn.DataParallel(model, device_ids=[0,1,2])\n",
    "    weights = torch.load(model_name)\n",
    "    \n",
    "    model.load_state_dict(weights['model'])\n",
    "    model.eval()\n",
    "\n",
    "    batch_size=600\n",
    "    test_dataset = LogDataset(TEST, None, train=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_pred=[]\n",
    "    for X in tqdm(test_loader):\n",
    "        X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "        pred = model(X)\n",
    "        test_pred += list(pred.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    test_pred = np.array([softmax(k) for k in test_pred])\n",
    "    return np.array(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "desirable-worcester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:09:16.800339Z",
     "start_time": "2021-05-14T19:41:29.642440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2365/2365 [05:31<00:00,  7.12it/s]\n",
      "100%|██████████| 2365/2365 [05:23<00:00,  7.31it/s]\n",
      "100%|██████████| 2365/2365 [05:23<00:00,  7.31it/s]\n",
      "100%|██████████| 2365/2365 [05:23<00:00,  7.31it/s]\n",
      "100%|██████████| 2365/2365 [05:23<00:00,  7.31it/s]\n"
     ]
    }
   ],
   "source": [
    "weights = sorted(glob('model/*'))\n",
    "test_preds = []\n",
    "\n",
    "for w in weights:\n",
    "    test_preds.append(output(w))\n",
    "test_preds = np.array(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accepted-grave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:32:52.713217Z",
     "start_time": "2021-05-15T06:32:48.431824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold : 0.7               [1002152  396509      34   12958      34    6535      26     668]\n"
     ]
    }
   ],
   "source": [
    "submit_level = return_level_array((test_preds[0]+test_preds[1]+test_preds[2]+test_preds[4])/4, 0.7, count_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "retained-bread",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:34:03.270970Z",
     "start_time": "2021-05-15T06:34:03.127123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418911</th>\n",
       "      <td>2418911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418912</th>\n",
       "      <td>2418912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418913</th>\n",
       "      <td>2418913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418914</th>\n",
       "      <td>2418914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418915</th>\n",
       "      <td>2418915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1418916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  level\n",
       "0        1000000      0\n",
       "1        1000001      0\n",
       "2        1000002      1\n",
       "3        1000003      0\n",
       "4        1000004      1\n",
       "...          ...    ...\n",
       "1418911  2418911      0\n",
       "1418912  2418912      0\n",
       "1418913  2418913      1\n",
       "1418914  2418914      0\n",
       "1418915  2418915      0\n",
       "\n",
       "[1418916 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('dataset/sample_submission.csv')\n",
    "submission['level'] = submit_level\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "straight-shooting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:34:06.213260Z",
     "start_time": "2021-05-15T06:34:04.842101Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
